import tempfile
import threading
import re
import io
import json
from dataclasses import dataclass, field
from typing import List, Optional
from datetime import datetime

import pandas as pd
import streamlit as st
from streamlit_autorefresh import st_autorefresh

from tiktok_scraper_core import run_tiktok_scraper
from fb_scraper_core import run_facebook_scraper


st.set_page_config(page_title="Social Comment Scraper", page_icon="üí¨", layout="wide")

# Custom CSS: gi·ªõi h·∫°n chi·ªÅu r·ªông 80%, cƒÉn gi·ªØa, v√† ·∫©n autorefresh iframe
st.markdown("""
<style>
    .block-container {
        max-width: 80% !important;
        margin: 0 auto;
    }
    /* ·∫®n iframe c·ªßa st_autorefresh */
    .st-key-scraper_refresh {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

st.title("üí¨ Social Comment Scraper")


# ========== VALIDATION FUNCTIONS ==========
def is_link_valid(link: str, platform: str) -> bool:
    """Ki·ªÉm tra link c√≥ ƒë√∫ng n·ªÅn t·∫£ng kh√¥ng"""
    l = (link or "").lower().strip()
    p = (platform or "").lower().strip()
    is_tiktok = "tiktok" in p
    is_facebook = "facebook" in p
    if is_tiktok:
        return "tiktok.com" in l and "facebook.com" not in l and "fb.watch" not in l and "fb.com" not in l
    if is_facebook:
        return ("facebook.com" in l or "fb.watch" in l or "fb.com" in l) and "tiktok.com" not in l
    return False


def is_cookie_valid(cookie_content: bytes, platform: str) -> bool:
    """Ki·ªÉm tra cookie c√≥ ƒë√∫ng n·ªÅn t·∫£ng kh√¥ng"""
    if not cookie_content:
        return False
    try:
        data = json.loads(cookie_content.decode("utf-8"))
        cookies = data.get("cookies") if isinstance(data, dict) else data
        if not isinstance(cookies, list):
            return False
        domains = []
        for c in cookies:
            if isinstance(c, dict):
                d = c.get("domain") or c.get("host") or c.get("url") or ""
                domains.append(str(d).lower())
        if not domains:
            return False
        p = (platform or "").lower().strip()
        is_tiktok = "tiktok" in p
        is_facebook = "facebook" in p
        if is_tiktok:
            return any("tiktok.com" in d or "tiktokv.com" in d for d in domains)
        if is_facebook:
            return any("facebook.com" in d or "fb.com" in d or "messenger.com" in d for d in domains)
        return False
    except Exception:
        return False


# ========== SHARED STATE (persist qua c√°c rerun) ==========
@dataclass
class ScraperState:
    stop_event: threading.Event = field(default_factory=threading.Event)
    data: List = field(default_factory=list)
    log_lines: List[str] = field(default_factory=list)
    status: str = "idle"  # idle, running, stopped, done
    thread: Optional[threading.Thread] = None
    platform: str = "TikTok"
    comment_count: int = 0  # ƒê·∫øm s·ªë b√¨nh lu·∫≠n ƒë√£ c√†o
    data_saved: bool = False  # ƒê√°nh d·∫•u ƒë√£ l∆∞u v√†o history ch∆∞a
    last_url: str = ""  # URL c·ªßa l·∫ßn c√†o hi·ªán t·∫°i


@st.cache_resource
def get_scraper_state():
    return ScraperState()


@st.cache_resource
def get_scraper_history():
    """L∆∞u tr·ªØ 5 l·∫ßn c√†o g·∫ßn nh·∫•t"""
    return []


state = get_scraper_state()
history = get_scraper_history()


def save_to_history(data: List, platform: str, url: str):
    """L∆∞u k·∫øt qu·∫£ v√†o l·ªãch s·ª≠ (gi·ªØ t·ªëi ƒëa 5 l·∫ßn g·∫ßn nh·∫•t)"""
    if not data:
        return
    
    entry = {
        "timestamp": datetime.now().strftime("%d/%m/%Y %H:%M"),
        "platform": platform,
        "url": url[:50] + "..." if len(url) > 50 else url,
        "count": len(data),
        "data": data.copy()
    }
    
    # Th√™m v√†o ƒë·∫ßu danh s√°ch
    history.insert(0, entry)
    
    # Gi·ªØ t·ªëi ƒëa 5 entries
    while len(history) > 5:
        history.pop()


def get_comment_count_from_logs():
    """Parse s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n t·ª´ logs (pattern: T·ªïng: X)"""
    for line in reversed(state.log_lines):
        match = re.search(r"T·ªïng:\s*(\d+)", line)
        if match:
            return int(match.group(1))
    return 0


def get_current_step():
    """X√°c ƒë·ªãnh b∆∞·ªõc hi·ªán t·∫°i t·ª´ logs"""
    if not state.log_lines:
        return "ƒêang chu·∫©n b·ªã..."
    
    # Duy·ªát t·ª´ cu·ªëi l√™n ƒë·ªÉ l·∫•y b∆∞·ªõc m·ªõi nh·∫•t
    for line in reversed(state.log_lines):
        line_lower = line.lower()
        
        # B∆∞·ªõc 1: Kh·ªüi t·∫°o tr√¨nh duy·ªát
        if "ƒëang kh·ªüi t·∫°o tr√¨nh duy·ªát" in line_lower:
            return "üöÄ ƒêang kh·ªüi t·∫°o tr√¨nh duy·ªát..."
        if "ƒë√£ kh·ªüi t·∫°o tr√¨nh duy·ªát" in line_lower:
            return "‚úÖ ƒê√£ kh·ªüi t·∫°o tr√¨nh duy·ªát"
            
        # B∆∞·ªõc 2: N·∫°p cookie
        if "ƒëang n·∫°p cookie" in line_lower:
            return "üç™ ƒêang n·∫°p cookie..."
        if "ƒë√£ n·∫°p" in line_lower and "cookie" in line_lower:
            return "‚úÖ ƒê√£ n·∫°p cookie"
        if "ch·∫°y kh√¥ng cookie" in line_lower:
            return "‚ö†Ô∏è Ch·∫°y kh√¥ng c√≥ cookie"
            
        # B∆∞·ªõc 3: Truy c·∫≠p b√†i vi·∫øt/video
        if "ƒëang v√†o" in line_lower or "ƒëang truy c·∫≠p" in line_lower:
            return "üåç ƒêang truy c·∫≠p link..."
            
        # B∆∞·ªõc 4: Chuy·ªÉn b·ªô l·ªçc (Facebook)
        if "ƒëang chuy·ªÉn b·ªô l·ªçc" in line_lower:
            return "üîÑ ƒêang chuy·ªÉn b·ªô l·ªçc b√¨nh lu·∫≠n..."
        if "ƒë√£ chuy·ªÉn b·ªô l·ªçc" in line_lower:
            return "‚úÖ ƒê√£ chuy·ªÉn b·ªô l·ªçc"
            
        # B∆∞·ªõc 5: ƒêang qu√©t comment
        if "b·∫Øt ƒë·∫ßu qu√©t" in line_lower:
            return "‚¨áÔ∏è ƒêang qu√©t b√¨nh lu·∫≠n..."
        if "t·ªïng:" in line_lower or "l·∫•y th√™m" in line_lower:
            return "‚¨áÔ∏è ƒêang qu√©t b√¨nh lu·∫≠n..."
            
        # ƒêang cu·ªôn
        if "ƒëang th·ª≠ cu·ªôn" in line_lower:
            return "‚è≥ ƒêang th·ª≠ t·∫£i th√™m..."
            
        # H·∫øt d·ªØ li·ªáu
        if "ƒë√£ h·∫øt d·ªØ li·ªáu" in line_lower or "ƒë√£ h·∫øt comment" in line_lower:
            return "üèÅ ƒê√£ qu√©t xong!"
    
    return "‚è≥ ƒêang x·ª≠ l√Ω..."


# ========== SCRAPER FUNCTION (ch·∫°y trong thread) ==========
def run_scraper_thread(url, cookie_path, platform_name, headless_mode):
    def log(msg):
        state.log_lines.append(str(msg))

    try:
        if platform_name == "Facebook":
            data = run_facebook_scraper(
                url,
                cookie_path,
                log,
                state.stop_event,
                headless=headless_mode,
            )
        else:
            data = run_tiktok_scraper(
                url,
                cookie_path,
                log,
                state.stop_event,
                headless=headless_mode,
            )

        state.data = data if data else []

        if state.stop_event.is_set():
            state.status = "stopped"
        else:
            state.status = "done"

    except Exception as e:
        state.log_lines.append(f"‚ùå L·ªói: {e}")
        state.status = "done"


# ========== AUTO REFRESH KHI ƒêANG CH·∫†Y ==========
if state.status == "running":
    st_autorefresh(interval=1500, limit=None, key="scraper_refresh")


# ========== LAYOUT 2 C·ªòT ==========
col_left, col_right = st.columns([6.5, 3.5], gap="large")

# ========== C·ªòT TR√ÅI: FORM NH·∫¨P LI·ªÜU ==========
with col_left:
    st.markdown("Ch·ªçn n·ªÅn t·∫£ng, nh·∫≠p link, v√† upload cookie JSON ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    
    is_running = state.status == "running"

    platform = st.selectbox(
        "N·ªÅn t·∫£ng",
        ["TikTok", "Facebook"],
        disabled=is_running,
        key="platform_select"
    )

    link_label = "Link video" if platform == "TikTok" else "Link b√†i vi·∫øt"
    link_placeholder = "https://www.tiktok.com/@user/video/..." if platform == "TikTok" else "https://www.facebook.com/...."
    target_url = st.text_input(link_label, placeholder=link_placeholder, disabled=is_running)

    cookie_file = st.file_uploader("Cookie JSON (b·∫Øt bu·ªôc)", type=["json"], disabled=is_running)
    headless = st.toggle("Ch·∫°y headless (d√†nh cho Cloud)", value=True, disabled=is_running)

    # ========== VALIDATION REALTIME ==========
    validation_errors = []

    if target_url.strip() and not is_link_valid(target_url, platform):
        validation_errors.append(f"‚ùå Link kh√¥ng ƒë√∫ng n·ªÅn t·∫£ng **{platform}**. Vui l√≤ng ki·ªÉm tra l·∫°i.")

    if cookie_file is not None and not is_cookie_valid(cookie_file.getvalue(), platform):
        validation_errors.append(f"‚ùå File cookie kh√¥ng ƒë√∫ng n·ªÅn t·∫£ng **{platform}**. Vui l√≤ng upload cookie c·ªßa {platform}.")

    for err in validation_errors:
        st.error(err)

    # ========== BUTTONS ==========
    if state.status == "running":
        if st.button("üõë D·ª´ng l·∫°i", type="secondary", use_container_width=True):
            state.stop_event.set()
            st.rerun()
    else:
        if st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu", type="primary", use_container_width=True):
            # Validate b·∫Øt bu·ªôc
            if not target_url.strip():
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p link.")
            elif cookie_file is None:
                st.warning("‚ö†Ô∏è Vui l√≤ng upload file cookie JSON.")
            elif not is_link_valid(target_url, platform):
                st.error(f"‚ùå Link kh√¥ng ƒë√∫ng n·ªÅn t·∫£ng **{platform}**.")
            elif not is_cookie_valid(cookie_file.getvalue(), platform):
                st.error(f"‚ùå File cookie kh√¥ng ƒë√∫ng n·ªÅn t·∫£ng **{platform}**.")
            else:
                # Reset state
                state.stop_event.clear()
                state.data = []
                state.log_lines = []
                state.status = "running"
                state.platform = platform
                state.comment_count = 0
                state.data_saved = False  # Reset flag ƒë·ªÉ cho ph√©p l∆∞u history
                state.last_url = target_url.strip()  # L∆∞u URL hi·ªán t·∫°i

                # Chu·∫©n b·ªã cookie
                cookie_path = None
                if cookie_file is not None:
                    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".json")
                    temp_file.write(cookie_file.getbuffer())
                    temp_file.flush()
                    temp_file.close()
                    cookie_path = temp_file.name

                # Ch·∫°y scraper trong thread ri√™ng
                thread = threading.Thread(
                    target=run_scraper_thread,
                    args=(target_url.strip(), cookie_path, platform, headless),
                    daemon=True
                )
                thread.start()
                state.thread = thread

                st.rerun()

    # ========== HI·ªÇN TH·ªä TR·∫†NG TH√ÅI ==========
    if state.status == "running":
        current_count = get_comment_count_from_logs()
        current_step = get_current_step()
        
        st.info(f"""
**{current_step}**

üìä ƒê√£ c√†o ƒë∆∞·ª£c: **{current_count}** b√¨nh lu·∫≠n
        """)
        st.caption("üîÑ T·ª± ƒë·ªông c·∫≠p nh·∫≠t m·ªói 1.5s | B·∫•m **üõë D·ª´ng l·∫°i** ƒë·ªÉ d·ª´ng v√† l∆∞u d·ªØ li·ªáu")

        # Ki·ªÉm tra thread c√≤n ch·∫°y kh√¥ng
        if state.thread and not state.thread.is_alive():
            st.rerun()

    elif state.status == "stopped":
        platform_name = state.platform.lower()
        if state.data:
            # L∆∞u v√†o history v√† reset v·ªÅ idle
            if not getattr(state, 'data_saved', False):
                save_to_history(state.data, state.platform, getattr(state, 'last_url', ''))
                state.data_saved = True
                
                # Hi·ªÉn th·ªã th√¥ng b√°o 1 l·∫ßn r·ªìi reset
                df = pd.DataFrame(state.data)
                st.warning(f"üõë ƒê√£ d·ª´ng theo y√™u c·∫ßu. L·∫•y ƒë∆∞·ª£c **{len(df)}** b√¨nh lu·∫≠n. ƒê√£ l∆∞u v√†o l·ªãch s·ª≠.")
                st.dataframe(df, use_container_width=True)
                
                # Export Excel
                buffer = io.BytesIO()
                df.to_excel(buffer, index=False, engine='openpyxl')
                buffer.seek(0)
                st.download_button(
                    "üì• T·∫£i Excel",
                    data=buffer,
                    file_name=f"{platform_name}_comments.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            else:
                # ƒê√£ l∆∞u r·ªìi, reset v·ªÅ idle
                state.status = "idle"
                state.data = []
                st.rerun()
        else:
            st.warning("üõë ƒê√£ d·ª´ng theo y√™u c·∫ßu. Ch∆∞a c√≥ d·ªØ li·ªáu.")
            state.status = "idle"

    elif state.status == "done":
        platform_name = state.platform.lower()
        if state.data:
            # L∆∞u v√†o history v√† reset v·ªÅ idle
            if not getattr(state, 'data_saved', False):
                save_to_history(state.data, state.platform, getattr(state, 'last_url', ''))
                state.data_saved = True
                
                # Hi·ªÉn th·ªã th√¥ng b√°o 1 l·∫ßn r·ªìi reset
                df = pd.DataFrame(state.data)
                st.success(f"‚úÖ Ho√†n th√†nh! ƒê√£ l·∫•y **{len(df)}** b√¨nh lu·∫≠n. ƒê√£ l∆∞u v√†o l·ªãch s·ª≠.")
                st.dataframe(df, use_container_width=True)
                
                # Export Excel
                buffer = io.BytesIO()
                df.to_excel(buffer, index=False, engine='openpyxl')
                buffer.seek(0)
                st.download_button(
                    "üì• T·∫£i Excel",
                    data=buffer,
                    file_name=f"{platform_name}_comments.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            else:
                # ƒê√£ l∆∞u r·ªìi, reset v·ªÅ idle
                state.status = "idle"
                state.data = []
                st.rerun()
        else:
            st.error("""
‚ö†Ô∏è **Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu.**

C√≥ th·ªÉ do cookie ƒë√£ h·∫øt h·∫°n ho·∫∑c kh√¥ng h·ª£p l·ªá. H√£y th·ª≠:
1. ƒêƒÉng xu·∫•t kh·ªèi t√†i kho·∫£n tr√™n tr√¨nh duy·ªát
2. ƒêƒÉng nh·∫≠p l·∫°i
3. Xu·∫•t cookie m·ªõi v√† th·ª≠ l·∫°i

**ƒê·ªëi v·ªõi TikTok:**
- Sau khi ƒëƒÉng nh·∫≠p n·∫øu kh√¥ng hi·ªán capcha th√¨ h√£y gi·ªØ tab ·ªü ƒë√≥ kho·∫£ng **5 - 10 ph√∫t** r·ªìi quay l·∫°i gi·∫£i captcha (n·∫øu c√≥)
- ƒê·ª£i gi·∫£i captcha xong h√£y l·∫•y cookie
- Cookie ch·ªâ c√≥ hi·ªáu l·ª±c sau khi ƒë√£ v∆∞·ª£t qua captcha
            """)
            state.status = "idle"

# ========== C·ªòT PH·∫¢I: L·ªäCH S·ª¨ 5 L·∫¶N C√ÄO G·∫¶N NH·∫§T ==========
with col_right:
    st.subheader("üìÇ L·ªãch s·ª≠ 5 l·∫ßn c√†o g·∫ßn nh·∫•t")
    
    if history:
        for i, entry in enumerate(history):
            with st.expander(f"**{entry['platform']}** - {entry['count']} b√¨nh lu·∫≠n - {entry['timestamp']}"):
                st.caption(f"üîó {entry['url']}")
                
                # Hi·ªÉn th·ªã preview data
                df_preview = pd.DataFrame(entry['data'])
                st.dataframe(df_preview.head(5), use_container_width=True)
                
                if len(entry['data']) > 5:
                    st.caption(f"... v√† {len(entry['data']) - 5} b√¨nh lu·∫≠n kh√°c")
                
                # N√∫t t·∫£i v·ªÅ
                buffer = io.BytesIO()
                pd.DataFrame(entry['data']).to_excel(buffer, index=False, engine='openpyxl')
                buffer.seek(0)
                st.download_button(
                    f"üì• T·∫£i Excel ({entry['count']} b√¨nh lu·∫≠n)",
                    data=buffer,
                    file_name=f"{entry['platform'].lower()}_comments_{entry['timestamp'].replace('/', '-').replace(':', '-').replace(' ', '_')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"download_history_{i}"
                )
    else:
        st.caption("Ch∆∞a c√≥ l·ªãch s·ª≠ c√†o n√†o.")
